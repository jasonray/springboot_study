global:
  scrape_interval: 60s # scrape targets every minute
  evaluation_interval: 1m # Evaluate rules every minute (also is the default)
  # scrape_timeout is set to the global default (10s).

# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
rule_files:
 - 'recording-rules.yml'

scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: 'prometheus'
    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.
    static_configs:
      - targets:
        - 'localhost:9090'
  - job_name: 'static-metrics'
    static_configs:
      - targets:
        - 'metrics:9100'
# NOTE: replacing __address__ causes prometheus attempt read metrics from it,  which is opposite of the intended
#   relabel_configs:
#     - source_labels: [__address__]
#       regex: "metrics:(.*)"
#       target_label: __address__
#       replacement: 'monitored_host:$1'
#       action: replace
# NOTE: does not work
    metric_relabel_configs:
      - source_labels: ['__name__', 'lone']
        separator: ';'
        action: drop
        regex: '(foo)'
      - source_labels: [__name__]
        regex: '^(node|promhttp_|go_|prometheus_|process_).*'
        separator: ';'
        action: drop
